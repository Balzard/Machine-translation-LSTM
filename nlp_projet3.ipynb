{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_projet3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kD4y5Zi0J69Z",
        "EShFfMRfOUxF",
        "zNB2n_ecynxz"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfXvbp6D-98d"
      },
      "source": [
        "import os, sys\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4MsEh4G_EJG"
      },
      "source": [
        "NUM_SENTENCES = 20000\n",
        "MAX_SENTENCE_LENGTH = 50\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_SIZE = 50\n",
        "VALIDATION_SPLIT = 0.1\n",
        "LSTM_NODES = 256\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOONEEyZDTy3"
      },
      "source": [
        "corpus = open(r\"/content/drive/MyDrive/NLP/fra_eng.txt\", encoding=\"utf-8\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD4y5Zi0J69Z"
      },
      "source": [
        "# Preprocess corpus\n",
        "\n",
        "The input to the encoder LSTM is the sentence in the original\n",
        "language; the input to the decoder LSTM is the sentence in the translated\n",
        "language with a start-of-sentence token. The output is the actual target\n",
        "sentence with an end-of-sentence token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzjmpj4TJZVg",
        "outputId": "dccea2d2-ac8b-4dfb-d465-5d4c247459b0"
      },
      "source": [
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "count = 0\n",
        "\n",
        "for line in corpus:\n",
        "  count += 1\n",
        "  if count > NUM_SENTENCES:\n",
        "    break\n",
        "  if '\\t' not in line:\n",
        "    continue\n",
        "  input_sentence, output = line.rstrip().split('\\t')[0:2]\n",
        "  output_sentence = output + ' <eos>'\n",
        "  output_sentence_input = '<sos> ' + output\n",
        "  input_sentences.append(input_sentence)\n",
        "  output_sentences.append(output_sentence)\n",
        "  output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"num samples input:\", len(input_sentences))\n",
        "print(\"num samples output:\", len(output_sentences))\n",
        "print(\"num samples output input:\", len(output_sentences_inputs))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num samples input: 20000\n",
            "num samples output: 20000\n",
            "num samples output input: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1d0UNq0Kvw-",
        "outputId": "29556e1b-bd42-47a5-9371-af47c136ec31"
      },
      "source": [
        "print(input_sentences[172])\n",
        "print(output_sentences[172])\n",
        "print(output_sentences_inputs[172])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm hit!\n",
            "Je suis touchée ! <eos>\n",
            "<sos> Je suis touchée !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb7acWgxK7Bt",
        "outputId": "8bc4b3e3-c39f-44d8-c78f-111aad9e28ae"
      },
      "source": [
        "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) # keep only the top 20 000 words in the sentences\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences) # vectorized input sentences\n",
        "word2idx_inputs = input_tokenizer.word_index # give integer value of a word for input\n",
        "id2words_inputs = input_tokenizer.index_word # give the word representing an integer\n",
        "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"Length of longest sentence in input: %g\" % max_input_len)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words in the input: 3511\n",
            "Length of longest sentence in input: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC2uKuHtMPQE",
        "outputId": "5975738a-de7c-4119-a78d-f72b2cc4049e"
      },
      "source": [
        "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "id2words_outputs = output_tokenizer.index_word\n",
        "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words in the output: 9523\n",
            "Length of longest sentence in the output: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPMFOkDpMQvD",
        "outputId": "cded5320-326f-4de5-99d6-fb3865f43b70"
      },
      "source": [
        "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
        "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
        "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_input_sequences.shape: (20000, 6)\n",
            "encoder_input_sequences[172]: [  0   0   0   0   6 615]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hk5HfkeMXZH",
        "outputId": "d1987594-beda-4522-9b9d-8523efbf16a1"
      },
      "source": [
        "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
        "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder_input_sequences.shape: (20000, 13)\n",
            "decoder_input_sequences[172]: [   2    3    6 2024    5    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80RzJtQeJRoY",
        "outputId": "08ed2a99-6d5f-43c5-f188-7e1b5a604175"
      },
      "source": [
        "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding=\"post\")\n",
        "print(\"decoder_output_sequences.shape:\", decoder_output_sequences.shape)\n",
        "print(\"decoder_output_sequences[172]:\", decoder_output_sequences[172])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder_output_sequences.shape: (20000, 13)\n",
            "decoder_output_sequences[172]: [   3    6 2024    5    1    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EShFfMRfOUxF"
      },
      "source": [
        "# Words embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feZ3LeB-MdID"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "# create dic of {word:vector_of_word}\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open(r'/content/drive/MyDrive/NLP/6471382cdd837544bf3ac72497a38715e845897d265b2b424b4761832009c837/glove.6B.50d.txt', encoding=\"utf8\")\n",
        "for line in glove_file:\n",
        " records = line.split()\n",
        " word = records[0]\n",
        " vector_dimensions = asarray(records[1:], dtype='float32')\n",
        " embeddings_dictionary[word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qtuOagkPEmO"
      },
      "source": [
        "# create a matrix where each row represent the integer value of a word\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
        "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
        "for word, index in word2idx_inputs.items():\n",
        " embedding_vector = embeddings_dictionary.get(word)\n",
        " if embedding_vector is not None:\n",
        "  embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iU7vwNzLntG",
        "outputId": "53e90043-6c36-491e-c18a-5420fad383b1"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3512, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0dABO2-LouE",
        "outputId": "ad90a764-a240-43a7-9615-95de8485e707"
      },
      "source": [
        "word2idx_inputs[\"hit\"]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "615"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U17jwifaLyOJ",
        "outputId": "99329145-0339-47eb-caa9-6aaa0c2da045"
      },
      "source": [
        "embedding_matrix[615]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.41659001, -0.47595999,  0.95744002,  0.27019   ,  0.17657   ,\n",
              "        0.24828   , -1.29869998,  0.53851002,  0.35336   ,  0.58221   ,\n",
              "       -0.33079001, -0.59680003, -0.97055   ,  0.72083998,  0.49463001,\n",
              "       -0.83398002,  0.12236   , -0.37237   , -1.45459998,  0.41384   ,\n",
              "       -0.36311001,  0.2202    ,  0.057482  , -0.24951001,  0.37654001,\n",
              "       -1.30610001,  0.22596   ,  0.47510001,  1.28600001, -0.62642998,\n",
              "        3.4058001 ,  0.18436   ,  1.26559997,  1.07410002,  0.3026    ,\n",
              "        0.31395   ,  0.33682999, -0.31895   ,  0.31911999,  0.37919   ,\n",
              "       -1.1652    ,  0.94625002, -0.044854  , -1.07790005, -0.16669001,\n",
              "        0.11604   , -0.11983   , -0.23662999,  0.29087999,  0.11071   ])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkCoZMYCNAQ9"
      },
      "source": [
        "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn6xB2t_XDjf",
        "outputId": "05eed10d-f2f0-4914-86b5-ad17d2d341a1"
      },
      "source": [
        "num_words"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3512"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FighAGhCPdCl"
      },
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqzcw-IhOOT3"
      },
      "source": [
        "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = LSTM(LSTM_NODES, return_state=True)\n",
        "encoder_outputs, h, c = encoder(x)\n",
        "encoder_states = [h, c]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld-JNTT3Vjqh"
      },
      "source": [
        "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
        "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
        "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
        "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rDZZbnH-yso"
      },
      "source": [
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ0oW3Lr-5Yu"
      },
      "source": [
        "model = Model([encoder_inputs_placeholder,\n",
        " decoder_inputs_placeholder], decoder_outputs)\n",
        "model.compile(\n",
        " optimizer='rmsprop',\n",
        " loss='categorical_crossentropy',\n",
        " metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "xkUPQbnHVzli",
        "outputId": "cd83d067-a95f-4d09-809f-9b5c2fa4b435"
      },
      "source": [
        "# You should train your neural net model as shown below:\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from numpy.random           import seed\n",
        "from tensorflow.random      import set_seed\n",
        "#-------------------------------------------------------------------------------\n",
        "# These steps would not be required if you were developing the auto-translation\n",
        "# project for yourself. These however, help reducing the variance induced by the\n",
        "# random neural weights assigned at the beginning of the gradient descent.\n",
        "# Which is useful when grading your answer\n",
        "#-------------------------------------------------------------------------------\n",
        "seed(42)\n",
        "set_seed(42)\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "# This sequence is used to feed the training process with batches that are not\n",
        "# all loaded in ram at once\n",
        "#------------------------------------------------------------------------------\n",
        "class LazyLoadedSequence(Sequence):\n",
        "  def __init__(self, begin, end):\n",
        "      self.begin      = begin        # beginning (included) of the data\n",
        "      self.end        = end          # end (excluded) of the data\n",
        "      self.nb_samples = end - begin  # number of data samples\n",
        "\n",
        "  def __len__(self):\n",
        "      # returns the number of batches of data\n",
        "      return np.ceil(self.nb_samples / BATCH_SIZE).astype(np.int)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      # returns the `idx`th batch of data\n",
        "      # (returns both inputs aka xs and outputs aka ys)\n",
        "      start   = self.begin + BATCH_SIZE * idx\n",
        "      end     = min(self.end, start + BATCH_SIZE)\n",
        "      #\n",
        "      enc_x   = encoder_input_sequences[start:end]\n",
        "      dec_x   = decoder_input_sequences[start:end]\n",
        "      one_hot = np.zeros((end-start, max_out_len, num_words_output), dtype='float16')\n",
        "      # now let us actually build the one hot encoded representation for each of\n",
        "      # the output sentences (in french)\n",
        "      for i, d in enumerate(decoder_output_sequences[start:end]):\n",
        "        for t, word in enumerate(d):\n",
        "          one_hot[i, t, word] = 1\n",
        "      # now return both the xs and the ys\n",
        "      return [enc_x, dec_x], one_hot\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Actually fit it with custom batches\n",
        "#------------------------------------------------------------------------------\n",
        "nb_sentences    = len(input_sentences)\n",
        "split_limit     = np.ceil(nb_sentences * (1 - VALIDATION_SPLIT)).astype(np.int)\n",
        "train_data      = LazyLoadedSequence(0, split_limit)\n",
        "validation_data = LazyLoadedSequence(split_limit, nb_sentences)\n",
        "\n",
        "r = model.fit(\n",
        "    train_data,\n",
        "    validation_data = validation_data,\n",
        "    epochs          = 20,\n",
        ")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "282/282 [==============================] - 149s 516ms/step - loss: 2.1670 - accuracy: 0.7110 - val_loss: 2.2034 - val_accuracy: 0.6894\n",
            "Epoch 2/20\n",
            "141/282 [==============>...............] - ETA: 1:08 - loss: 1.7461 - accuracy: 0.7383"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-0c48d566d363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mepochs\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5xtuDpcIrnk"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q943gbbpAw3d"
      },
      "source": [
        "#model.save(\"/content/drive/MyDrive/NLP/model_translation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNB2n_ecynxz"
      },
      "source": [
        "# Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTYF1fsLplGu"
      },
      "source": [
        "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xthQhRtnyrpn"
      },
      "source": [
        "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
        "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ8CycqBytpS"
      },
      "source": [
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH6wr2V5yvAU"
      },
      "source": [
        "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9D0rDNKywAE"
      },
      "source": [
        "decoder_states = [h, c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdLWUNEOyyGk"
      },
      "source": [
        "decoder_model = Model(\n",
        " [decoder_inputs_single] + decoder_states_inputs,\n",
        " [decoder_outputs] + decoder_states\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_TS4C_tzKEx"
      },
      "source": [
        "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DAdSZO4CSUy"
      },
      "source": [
        "# Translate sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtxDkDqBzMPX"
      },
      "source": [
        "def translate_sentence(sentence: str, encoder, decoder, word2idx_outputs, idx2word_target) -> str:\n",
        "  input_seq = pad_sequences(input_tokenizer.texts_to_sequences([sentence]), maxlen=max_input_len)\n",
        "  states_value = encoder.predict(input_seq)\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "  eos = word2idx_outputs['<eos>']\n",
        "  output_sentence = []\n",
        "  for _ in range(max_out_len):\n",
        "    output_tokens, h, c = decoder.predict([target_seq] + states_value)\n",
        "    idx = np.argmax(output_tokens[0, 0, :])\n",
        "    if eos == idx:\n",
        "      break\n",
        "    word = ''\n",
        "    if idx > 0:\n",
        "      word = idx2word_target[idx]\n",
        "      output_sentence.append(word)\n",
        "    target_seq[0, 0] = idx\n",
        "    states_value = [h, c]\n",
        "  return ' '.join(output_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q4pDGZofy-p"
      },
      "source": [
        "translate_sentence(\"I'm a lawyer.\", encoder_model, decoder_model, word2idx_outputs, idx2word_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScSrMnHagnSx"
      },
      "source": [
        "translate_sentence(\"Is anybody hurt?\", encoder_model, decoder_model, word2idx_outputs, idx2word_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYdsD6CXgnIT"
      },
      "source": [
        "translate_sentence(\"I'm concentrating.\", encoder_model, decoder_model, word2idx_outputs, idx2word_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ozrz15wBzta"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dhM9aR2B_Ty"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUdzb46pBznU"
      },
      "source": [
        "reference = [['this', 'looks', 'highly', 'satisfactory', '<eos>'], ['this', 'looks', 'good', 'indeed', '<eos>' ]]\n",
        "candidate = ['this', 'is', 'very', 'good', 'indeed', '<eos>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQrJOeKT8N0a"
      },
      "source": [
        "chencherry = SmoothingFunction()\n",
        "sentence_bleu(reference, candidate,smoothing_function=chencherry.method1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GACUQ6AIGCr"
      },
      "source": [
        "sum = 0\n",
        "weights = (1./2., 1./2.)\n",
        "for i in input_sentence[split_limit:]:\n",
        "  hyp = output_tokenizer.texts_to_sequences([translate_sentence(i,encoder, decoder,word2idx_outputs, idx2word_target ) + \"<eos>\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0jgOiZwI82F"
      },
      "source": [
        "output_tokenizer.texts_to_sequences([translate_sentence(input_sentences[1],encoder, decoder,word2idx_outputs, idx2word_target )])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hub3nUjtD2fp"
      },
      "source": [
        "validation_output_sentences = output_sentences[split_limit:]\n",
        "references = [output_tokenizer.texts_to_sequences([i]) for i in validation_output_sentences]\n",
        "for i in range(len(references)):\n",
        "  for j in range(len(references[i][0])):\n",
        "    references[i][0][j] = id2words_outputs[references[i][0][j]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7fyFzJpmlky"
      },
      "source": [
        "references[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X80DJ4I8pqY1"
      },
      "source": [
        "hypotheses = [translate_sentence(i, encoder_model, decoder_model, word2idx_outputs, idx2word_target) for i in input_sentences[split_limit:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lda9b_XwyjuI"
      },
      "source": [
        "hyp = [output_tokenizer.texts_to_sequences([i]) for i in hypotheses]\n",
        "#hyp = [output_tokenizer.texts_to_sequences([i]) for i in tmp]\n",
        "for i in range(len(hyp)):\n",
        "  for j in range(len(hyp[i][0])):\n",
        "    hyp[i][0][j] = id2words_outputs[hyp[i][0][j]]\n",
        "  hyp[i][0].append(\"<eos>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezMXffIq2w6P"
      },
      "source": [
        "hyp[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J76gAA2L3VGT"
      },
      "source": [
        "hypotheses[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9guObYZSEY4l"
      },
      "source": [
        "weights = (1./2., 1./2.)\n",
        "corpus_bleu(references, hypotheses, weights, smoothing_function=chencherry.method1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}